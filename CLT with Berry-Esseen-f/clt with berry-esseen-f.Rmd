---
title: "CLT simulation with Berry-Esseen"
author: "Levi Lopes de Lima"
date: "2025-08-05"
output:
  html_document: default
link-citations: yes
bibliography: "feyn-notes.bib"
---

#### Details about the lab

Here we will provide a few simulations in order to illustrate the validity of a fundamental result in Probability Theory, namely, the **Central Limit Theorem** (CLT). After plotting the CDFs (cummulative distribution functions) and PDFs (probability density functions) of the tree parent distributions to be used as "initial configurations" (**uniform**, **exponential** and **Gamma**), which allows us to appreciate their theoretical distinctions (both in **shape** and **skewness**), we proceed to run simulations based on a R function which generates the plots describing the convergence (not only at the PDF but, more importantly, at the CDF level) and also annotates them with key statistics, specially the standard deviation, which we adopt as a measure of convergence toward a normal and hence may be used to interpret how the parameters affect the outcome. In order to better grasp how the normal approximation depends so heavily on the shape of the parent distribution, we run another simulation (this time only for the **uniform** and the **exponentional**) in which the whole process is re-examined in the light of the celebrated **Berry-Esseen theorem**, a key result formalizing what has already been observed visually in our previous simulation: the rate of convergence of the sample mean to a normal distribution is fundamentally governed by the skewness of the original population (as measured by its **shape factor**).


All the math needed to understand the simulation may be found in [@delima2025probab], which is available [here](https://drive.google.com/file/d/1kgsn95hrqW-zVhsVfMvmMP0aabRCtCeP/view "my notes").


#### A glimpse at the theory

Let us recall the statement of the **Central Limit Theorem** (CLT), which applies to an infinite **random sample**, that is, an infinite list $\{X_n\}_{n=1}^{+\infty}$ of random variables which are both **indepenent** and **identically distributed**. Let us assume that $\mathbb E(X_n)=\mu$ and ${\rm var}(X_n)=\sigma^2$ are both finite, which allows us to form the **standartization** 
\[
Z_n=\frac{\overline{X}_n-\mu}{\sigma/\sqrt{n}}
\] 
of the **sample mean**
\[
\overline{X}_n=\frac{X_1+\cdots+X_n}{n}. 
\]
Under these conditions, CLT says that $Z_n\stackrel{d}{\to}Z$, where $Z\sim\mathcal N(0,1)$, the standard normal distribution and the convergence is in **distribution**: one has the pointwise convergence
\[
F_{X_n}\to \Phi, 
\]
where in general $F_X:\mathbb R\to[0,1]$ is the cumulative distribution function (CDF) of $X$, so in partickular,  
\[
\Phi(x):=F_Z(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{-t^2/2}dt. 
\]
Put in another way, there holds the following asymptotic limit for the sample mean
\[
\overline X_n \approx_{n\to +\infty} \mathcal N(\mu,\sigma^2/n).
\]
This is the version of CLT that we will simulate below. More precisely, for certain  choices of $n$, the **sample size**, we will simulate the sample mean $\overline X_n$ of a few parent distributions and then compare the corresponding histograms to the theoretical normal distributions $\mathcal N(\mu,\sigma^2/n)$ which $\overline X_n$ is supposed to follow asymptotically (that is, as $n\to+\infty$). 




#### Loading the packages  

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
```


#### Looking at the parent distributions

We first examine the parent distributions whose shapes will provide the "raw ingredients" for implement the CLT, namely, 

\[
\begin{array}{lccccc}
 & | & {\rm pdf}  & | & {\rm mean}=\mu & | & {\rm  sd}=\sigma \\
{\rm uniform}  & | & {\bf 1}_{[0,10]}/10   & | &  5  & |  & 5\sqrt{3}/3 \approx 2.887 \\ 
{\rm exponential} & | &  0.5 e^{-0.5 x}{\bf 1}_{[0,+\infty]} & |  & 2 & | & 2 \\
{\rm gamma}  & | & \frac{1}{4\Gamma(2)}xe^{-x/2}{\bf 1}_{[0,+\infty]} &  | & 4  & |  & 2\sqrt{2} \approx 2.828
\end{array}
\]
where ${\rm sd}$ is the **standard deviation** and ${\bf 1}$ denotes an
**indication function**. Note that the parameters for **uniform** and **Gamma** have been chosen so that the ${\rm sd}$s are about the same. In particular, the sample mean ${\rm sd}$s, namely $\sigma/\sqrt{n}$, essentially agree for $n$ large.  

We start by plotting the PDFs and CDFs for these distributions by means of a function, `plot_theoretical_distributions()`, which delivers a list `theoretical_plots` with two ggplot objects: `pdf_plots` and `cdf_plots`.


```{r}
library(tidyverse)

plot_theoretical_distributions <- function(
  params = list(
    uniform     = list(min = 0, max = 10),
    exponential = list(rate = 0.5),
    gamma       = list(shape = 2, scale = 2)
  ),
  n_points   = 600,
  same_scale = FALSE
) {
  # Supported distributions
  dist_suffix <- function(name) {
    switch(tolower(name),
           "uniform"     = "unif",
           "exponential" = "exp",
           "gamma"       = "gamma",
           NULL
    )
  }

  get_funs <- function(name) {
    suf <- dist_suffix(name)
    if (is.null(suf)) return(NULL)
    list(
      d = get(paste0("d", suf), mode = "function"),
      p = get(paste0("p", suf), mode = "function"),
      q = get(paste0("q", suf), mode = "function"),
      suf = suf
    )
  }

  # Build data per distribution
  dist_dfs <- purrr::imap(params, function(arg_list, name) {
    f <- get_funs(name)
    if (is.null(f)) return(NULL)

    q_lo <- suppressWarnings(
      tryCatch(do.call(f$q, c(list(p = 0.001), arg_list)), error = function(e) NA_real_)
    )
    q_hi <- suppressWarnings(
      tryCatch(do.call(f$q, c(list(p = 0.999), arg_list)), error = function(e) NA_real_)
    )

    if (is.na(q_lo) || is.na(q_hi) || !is.finite(q_lo) || !is.finite(q_hi)) {
      if (f$suf == "unif") {
        q_lo <- arg_list$min %||% 0
        q_hi <- arg_list$max %||% 1
      } else {
        q_lo <- 0
        q_hi <- 10
      }
    }

    rng  <- q_hi - q_lo
    pad  <- max(0.05 * rng, 1e-8)
    xmin <- q_lo - pad
    xmax <- q_hi + pad

    tibble(
      distribution = name,
      x = seq(xmin, xmax, length.out = n_points)
    ) %>%
      mutate(
        pdf_y = do.call(f$d, c(list(x = x), arg_list)),
        cdf_y = do.call(f$p, c(list(q = x), arg_list))
      )
  }) %>% compact()

  # Combine and set fixed order
  curve_data <- bind_rows(dist_dfs) %>%
    mutate(
      Distribution = factor(
        stringr::str_to_title(distribution),
        levels = c("Uniform", "Exponential", "Gamma")
      )
    )

  base_aes <- aes(x = x, group = Distribution, color = Distribution)

  p_pdf <- ggplot(curve_data, aes(y = pdf_y)) +
    geom_line(base_aes, linewidth = 1.2) +
    facet_wrap(~ Distribution, scales = if (same_scale) "fixed" else "free") +
    labs(
      title    = "Probability Density Functions (PDFs)",
      subtitle = "Uniform, Exponential, and Gamma",
      x = "Value", y = "Density", color = "Distribution"
    ) +
    theme_minimal(base_size = 12) +
    theme(plot.title = element_text(hjust = 0.5),
          plot.subtitle = element_text(hjust = 0.5))

  p_cdf <- ggplot(curve_data, aes(y = cdf_y)) +
    geom_line(base_aes, linewidth = 1.2) +
    facet_wrap(~ Distribution, scales = if (same_scale) "fixed" else "free") +
    labs(
      title    = "Cumulative Distribution Functions (CDFs)",
      subtitle = "Uniform, Exponential, and Gamma",
      x = "Value", y = "Cumulative Probability", color = "Distribution"
    ) +
    theme_minimal(base_size = 12) +
    theme(plot.title = element_text(hjust = 0.5),
          plot.subtitle = element_text(hjust = 0.5))

  list(
    data = curve_data,
    pdf_plots = p_pdf,
    cdf_plots = p_cdf
  )
}

# Example
params1 <- list(
  uniform     = list(min = 0, max = 10),
  exponential = list(rate = 0.5),
  gamma       = list(shape = 2, scale = 2)
)

theoretical_plots <- plot_theoretical_distributions(params = params1)
# theoretical_plots$pdf_plots
# theoretical_plots$cdf_plots



```


With the list at hand, we may now call the plots. 

```{r}
theoretical_plots$cdf_plots
theoretical_plots$pdf_plots
```


The three distinct shapes of the starting distributions are now apparent from the PDFs plots:


  - **Uniform**: A perfectly flat, symmetric (around its mean $= 5$) but non-bell-shaped distribution;
  
  - **Exponential**: A highly right-skewed distribution;
  
  - **Gamma**: A moderately right-skewed distribution.

As far as the accumulation of probabilities goes, the CDF of the **uniform** displays a constant rate (an inclined straight line), whereas the CDFs of the **exponential** an  **Gamma** show that the probability accumulates very quickly for small values and then levels off, which is another way to confirm their **right-skewness**. Taken together, these plots now serve as the ideal "initial condition" picture for our CLT simulation, clearly showing how, even for arbitrary **non-normal** shapes, the application of the miraculous device of sampling and averaging will eventually produce an asymptotically normal sampling distribution (as the sample size $n$ grows indefinitely).

We now look at the code for the simulation, which involves a function `plot_clt_final_version()` depending on three parameters, namely, 

  - `population_size`, the number of observations in the parent populations,
  - `sample_size`, the number of observations to draw for each sample ($n$),  
  - `num_samples`, the number of samples to draw and calculate the mean.

and returns a ggplot object with all requested features:

  - A histogram of simulated sample means;
  - The theoretical normal curve $\mathcal N(\mu, \sigma^2/n)$ in red;
  - Short vertical solid segments indicating the observed (orange) vs. theoretical (green) mean;
  - Short vertical dashed segments indicating the observed (orange) vs. theoretical (green) spread;
  - A text annotation comparing the observed and theoretical standard deviation (**sd**).


```{r}

plot_clt_final_version <- function(population_size = 10000, sample_size = 50, num_samples = 10000) {
  set.seed(42) # for reproducibility
  # Create populations and store true parameters
  population_uniform <- runif(population_size, 0, 10)
  population_exponential <- rexp(population_size, rate = 0.5)
  population_gamma <- rgamma(population_size, shape = 2, scale = 2)
  pop_stats <- data.frame(
    distribution = factor(c("Uniform", "Exponential", "Gamma"), levels = c("Uniform", "Exponential", "Gamma")),
    pop_mean = c(mean(population_uniform), mean(population_exponential), mean(population_gamma)),
    pop_sd = c(sd(population_uniform), sd(population_exponential), sd(population_gamma))
  )
  # Simulate sample means
  simulate_means <- function(population, n_samples, samp_size) {
    replicate(n_samples, mean(sample(population, size = samp_size, replace = TRUE)))
  }
  plot_data <- data.frame(
    means = c(
      simulate_means(population_uniform, num_samples, sample_size),
      simulate_means(population_exponential, num_samples, sample_size),
      simulate_means(population_gamma, num_samples, sample_size)
    ),
    distribution = factor(rep(c("Uniform", "Exponential", "Gamma"), each = num_samples), levels = c("Uniform", "Exponential", "Gamma"))
  )
  # Create data for annotations plus vertical lines
  stats_summary <- plot_data %>%
    group_by(distribution) %>%
    summarise(
      obs_mean = mean(means),
      obs_var = var(means),
      .groups = 'drop'
    ) %>%
    left_join(pop_stats, by = "distribution") %>%
    mutate(
      obs_sd = sqrt(obs_var),
      theoretical_sd = pop_sd / sqrt(sample_size),
      peak_y = dnorm(pop_mean, mean = pop_mean, sd = theoretical_sd),
      segment_y_end = peak_y / 3,
      label = sprintf(
        "Observed SD: %.4f\nTheoretical SD (σ/√n): %.4f",
        obs_sd, theoretical_sd
      )
    )
 # Create data for the theoretical red durve
  curve_data <- plot_data %>%
    group_by(distribution) %>%
    reframe(x_coords = seq(min(means), max(means), length.out = 200)) %>%
    left_join(pop_stats, by = "distribution") %>%
    mutate(
      y_coords = dnorm(x_coords, mean = pop_mean, sd = pop_sd / sqrt(sample_size))
    )
  # Generate the final plot
  p <- ggplot(plot_data, aes(x = means)) +
    geom_histogram(aes(y = after_stat(density)), bins = 40, fill = "lightblue", color = "black", alpha = 0.7) +
    geom_line(data = curve_data, aes(x = x_coords, y = y_coords), color = "red", linewidth = 1) +
  # Include the vertical line segments for spread
    geom_segment(data = stats_summary, aes(x = pop_mean, y = 0, xend = pop_mean, yend = segment_y_end, color = "Theoretical"), linetype = "solid", linewidth = 1) +
    geom_segment(data = stats_summary, aes(x = pop_mean - theoretical_sd, y = 0, xend = pop_mean - theoretical_sd, yend = segment_y_end, color = "Theoretical"), linetype = "dashed", linewidth = 1) +
    geom_segment(data = stats_summary, aes(x = pop_mean + theoretical_sd, y = 0, xend = pop_mean + theoretical_sd, yend = segment_y_end, color = "Theoretical"), linetype = "dashed", linewidth = 1) +
    geom_segment(data = stats_summary, aes(x = obs_mean, y = 0, xend = obs_mean, yend = segment_y_end, color = "Observed"), linetype = "solid", linewidth = 1) +
    geom_segment(data = stats_summary, aes(x = obs_mean - obs_sd, y = 0, xend = obs_mean - obs_sd, yend = segment_y_end, color = "Observed"), linetype = "dashed", linewidth = 1) +
    geom_segment(data = stats_summary, aes(x = obs_mean + obs_sd, y = 0, xend = obs_mean + obs_sd, yend = segment_y_end, color = "Observed"), linetype = "dashed", linewidth = 1) +
geom_text(data = stats_summary, aes(label = label), x = Inf, y = Inf, hjust = 1.05, vjust = 1.05, size = 3.5) +
    facet_wrap(~ distribution, scales = "free") +
    scale_color_manual(name = "Spread (SD) Indicators", values = c("Theoretical" = "darkgreen", "Observed" = "darkorange")) +
    labs(
      title = "Distribution of Sample Means vs. Theoretical Normal Curve (CLT)",
      subtitle = paste("Solid lines are Means, Dashed lines are plus-minus one Std. Dev. | n =", sample_size),
      x = "Sample Mean",
      y = "Density"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 16),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      legend.position = "bottom"
    )
  # The final output
  return(p)
}
```


With the code at hand we may now run the simulation. We keep both `population_size` and `num_samples` as in default and only change `sample_size` ($n$). We start with $n=5$.

```{r}
plot_clt_final_version(sample_size = 5)
```


With just a few iterations we already see that the **uniform** is the distribution that best fits the theoretical normal distribution in red, notably if compared to the **Gamma**, which essentially has the same spread (as measured by sd). In fact, we detect a slight shift to the left of the histograms of **exponential** and **Gamma**, which at first sight should be attributed to their skewness.

We now look at three more iterations ...

```{r}
plot_clt_final_version(sample_size = 50)
```

```{r}
plot_clt_final_version(sample_size = 100)
```

```{r}
plot_clt_final_version(sample_size = 200)
```

...so that with $n=200$ we see an almost identical superposition of the theoretical and observed plots, with the observed and theoretical ${\rm sd}$s agreeing up to at least two decimal places. Clearly, this **visual** agreement in shape together with the **numerical** evidence provided by the virtual coincidence of the spreads (standard deviations) point toward a quite effective empirical confirmation of CLT.

#### CLT from the CDF viewpoint

We should have in mind, however, that the confirmation of CLT via the simulation above is a bit misleading because
the formal statement of the CLT is about the pointwise convergence of the CDF of the standardized sample mean to the CDF of a standard normal distribution.
Thus, redoing the simulation above at the CDF level turns out to be a more rigorous and insightful way to verify the theorem. Put in another way, the PDF/histogram approach is intuitive for "shape," but the CDF comparison, to be implemented below, is what mathematically underpins the concept.

With this goal in mind, we present below a separate function designed specifically to create this comparison. It will feature:

  - The empirical CDF (ECDF) of the simulated sample means. This will be a blue step-function representing the simulation's actual results.
  
  - The theoretical CDF of the limiting normal distribution $\mathcal N(\mu,\sigma^2/n)$. This will be a smooth red curve representing the CLT prediction.
  
Thus, the closer the blue steps hug the red curve, the better the convergence.

Here is the code, a simple adaptation of the previous one...

```{r}
plot_clt_cdf_convergence <- function(population_size = 10000, sample_size = 50, num_samples = 10000) {
  
  set.seed(42) # for reproducibility
  
  population_uniform <- runif(population_size, 0, 10)
  population_exponential <- rexp(population_size, rate = 0.5)
  population_gamma <- rgamma(population_size, shape = 2, scale = 2)
  
  pop_stats <- data.frame(
    distribution = factor(c("Uniform", "Exponential", "Gamma"), levels = c("Uniform", "Exponential", "Gamma")),
    pop_mean = c(mean(population_uniform), mean(population_exponential), mean(population_gamma)),
    pop_sd = c(sd(population_uniform), sd(population_exponential), sd(population_gamma))
  )
  
  simulate_means <- function(population, n_samples, samp_size) {
    replicate(n_samples, mean(sample(population, size = samp_size, replace = TRUE)))
  }
  
  plot_data <- data.frame(
    means = c(
      simulate_means(population_uniform, num_samples, sample_size),
      simulate_means(population_exponential, num_samples, sample_size),
      simulate_means(population_gamma, num_samples, sample_size)
    ),
    distribution = factor(rep(c("Uniform", "Exponential", "Gamma"), each = num_samples), levels = c("Uniform", "Exponential", "Gamma"))
  )
  
  theoretical_cdf_data <- plot_data %>%
    group_by(distribution) %>%
    reframe(x_coords = seq(min(means), max(means), length.out = 500)) %>%
    left_join(pop_stats, by = "distribution") %>%
    mutate(
      y_coords = pnorm(x_coords, mean = pop_mean, sd = pop_sd / sqrt(sample_size))
    )

  p <- ggplot(plot_data, aes(x = means)) +
    
    stat_ecdf(aes(color = "Empirical CDF"), geom = "step", linewidth = 1) +
    
    geom_line(
      data = theoretical_cdf_data,
      aes(x = x_coords, y = y_coords, color = "Theoretical Normal CDF"),
      linewidth = 1.2,
      linetype = "dashed"
    ) +
    
    facet_wrap(~ distribution, scales = "free_x") +
    
    # 5. Control appearance and add legend 
    scale_color_manual(
      name = "Distribution Type",
      values = c("Empirical CDF" = "dodgerblue", "Theoretical Normal CDF" = "red")
    ) +
    labs(
      title = "CLT Convergence of Cumulative Distribution Functions (CDFs)",
      subtitle = paste("Comparing simulated ECDF to the theoretical Normal CDF for n =", sample_size),
      x = "Sample Mean",
      y = "Cumulative Probability"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 16),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      legend.position = "bottom"
    )
  return(p)
}
```

We will run the simulation only for two values of $n$. First, $n=5$:

```{r}
plot_clt_cdf_convergence(sample_size = 5)
```

For the **uniform**, the fit turns out to be already quite good (certainly because the parent distribution is symmetric). On the other hand,
for the **exponential** and **Gamma**, clear deviations are apparent, as the blue step-function fails to perfectly align with the red curve and gaps are visible, particularly in the lower and upper tails. This shows that for a small $n$, the simulation from a skewed parent distribution fails to deliver a reasonably complete convergence as the sampling distribution is not yet "sufficiently" normal.

Let us now try $n=200$.

```{r}
plot_clt_cdf_convergence(sample_size = 200)
```

We now see in all three plots the blue step-function (Empirical CDF) hugging the red dashed curve (Theoretical CDF) extremely closely, turning them almost indistinguishable, which provides a powerful visual proof that for a large $n$ the distribution of the sample means has asymptotically converged to the normal distribution, as predicted by the CLT. 



#### Simulating CLT with a view toward Berry-Esseen

As a final simulation, we now seek to explain how the **skewness** of the parent distributions influences the convergence rate to a normal. The key theoretical tool is the celebrated **Berry-Esseen theorem**, which says that under the conditions of CLT, and assuming further that the **third centered moment** $\rho:=\mathbb E(|X_n-\mu|^3)$ is finite as well, there holds, for any $x\in\mathbb R$ and $n\geq 1$, 
\[
\sup_{x\in\mathbb R}\left|F_{n}(x)-\Phi(x)\right|\leq \frac{C\varepsilon}{\sqrt{n}}, 
\]
where $F_n=F_{X_n}$ and $\varepsilon=\rho/\sigma^3$ is the **standardized skewness** (or **shape factor**) of the parent distribution. The value of the universal constant $C$
 (the Berry-Esseen constant) has been refined over the years, with modern bounds placing it around $0.4748$. Thus, apart from $C$ and the expected term $1/\sqrt{n}$ involving the sample size, the rate of convergence is theoretically determined by $\varepsilon$. Clearly, this key result goes a long way toward explaining **why** the quality of the normal approximation depends so heavily on the shape of the parent distribution.
 
 
We now simulate and visualize this with the appropriate code (this time for the **uniform** and **exponential** only), which features:

  - The maximum observed error vs. the theoretical bound (both as a function of $n$).
  -  A grid of "snapshot" plots showing the error term $F_n(x) - \Phi(x)$ converging to zero as the  sample size $n$ increases (the values of $n$ can be chosen at will but we take $n=20,100,200$).
  

```{r}
simulate_berry_esseen_snapshots <- function(
    distributions_to_test = c("Uniform", "Exponential"),
    n_values = c(5, 10, 20, 50, 100, 200, 500),
    num_simulations = 10000,
    n_values_for_x_plot = c(20, 100, 200) # Updated argument
  ) {
  
  # Parameter setup plus moment calculation
  set.seed(42)
  params <- list(Uniform = list(min = 0, max = 10), Exponential = list(rate = 0.5))
  C_BE <- 0.4748
  message("Displaying exact theoretical moments (just for the record...)")
  moment_calculations <- lapply(distributions_to_test, function(dist) {
    if (dist == "Uniform") {
      a <- params$Uniform$min; b <- params$Uniform$max; mu <- (a + b) / 2; sigma <- (b - a) / sqrt(12)
      rho_integrand <- function(x) abs(x - mu)^3 * dunif(x, a, b); rho <- integrate(rho_integrand, lower = a, upper = b)$value
    } else if (dist == "Exponential") {
      lambda <- params$Exponential$rate; mu <- 1 / lambda; sigma <- 1 / lambda
      rho_integrand <- function(x) abs(x - mu)^3 * dexp(x, rate = lambda); rho <- integrate(rho_integrand, lower = 0, upper = Inf)$value
    }
    data.frame(distribution = dist, mu = mu, sigma = sigma, rho = rho, shape_factor = rho / (sigma^3))
  })
  pop_moments <- bind_rows(moment_calculations)
  print(pop_moments)

  # Simulation and generation of Plot 1 
  simulation_grid <- crossing(distribution = distributions_to_test, n = n_values)
  results_list <- pmap(simulation_grid, function(distribution, n) {
    moments <- pop_moments %>% filter(distribution == !!distribution)
    z_values <- replicate(num_simulations, {
      sample_data <- switch(distribution, "Uniform" = runif(n, params$Uniform$min, params$Uniform$max), "Exponential" = rexp(n, rate = params$Exponential$rate))
      (mean(sample_data) - moments$mu) / (moments$sigma / sqrt(n))
    })
    ecdf_z <- ecdf(z_values); knots_z <- knots(ecdf_z); observed_error <- max(abs(ecdf_z(knots_z) - pnorm(knots_z)))
    theoretical_bound <- C_BE * moments$shape_factor * (1 / sqrt(n))
    data.frame(distribution = distribution, n = n, observed_error = observed_error, theoretical_bound = theoretical_bound)
  })
  results_df <- bind_rows(results_list)
  plot1_data <- results_df %>% pivot_longer(c("observed_error", "theoretical_bound"), names_to = "type", values_to = "error_value")
  p_error_vs_n <- ggplot(plot1_data, aes(x = n, y = error_value, color = type, shape = type)) + geom_line(aes(linetype = type), linewidth = 1) + geom_point(size = 3) +
    facet_wrap(~ distribution, scales = "free_y") + scale_y_log10() + scale_color_manual(values = c(observed_error = "blue", theoretical_bound = "red"), labels = c("Observed Max Error", "Theoretical Bound")) +
    scale_shape_manual(values = c(observed_error = 16, theoretical_bound = 17), labels = c("Observed Max Error", "Theoretical Bound")) +
    scale_linetype_manual(values = c(observed_error = "solid", theoretical_bound = "dashed"), labels = c("Observed Max Error", "Theoretical Bound")) +
    labs(title = "Berry-Esseen Theorem: Observed Error vs. Theoretical Bound", subtitle = "Note: y-axis is on a log scale to show the convergence rate.", x = "Sample Size (n)", y = "Maximum Error |F_n(x) - Φ(x)| (log scale)") +
    theme_minimal() + theme(legend.title = element_blank(), legend.position = "bottom")

  # Simulation for plot 2
  
  snapshot_grid <- crossing(distribution = distributions_to_test, n = n_values_for_x_plot)
  
  error_curve_list <- pmap(snapshot_grid, function(distribution, n) {
    moments <- pop_moments %>% filter(distribution == !!distribution)
    z_values_fixed_n <- replicate(num_simulations, {
      sample_data <- switch(distribution, "Uniform" = runif(n, params$Uniform$min, params$Uniform$max), "Exponential" = rexp(n, rate = params$Exponential$rate))
      (mean(sample_data) - moments$mu) / (moments$sigma / sqrt(n))
    })
    ecdf_z <- ecdf(z_values_fixed_n)
    x_grid <- seq(min(z_values_fixed_n), max(z_values_fixed_n), length.out = 1000)
    
    data.frame(distribution = distribution, n = n, x = x_grid, error_term = ecdf_z(x_grid) - pnorm(x_grid))
  })
  
  error_curve_df <- bind_rows(error_curve_list)
  
  #  Generate Plot 2 
  p_error_vs_x <- ggplot(error_curve_df, aes(x = x, y = error_term)) +
    geom_line(color = "purple", linewidth = 1) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    
    facet_grid(distribution ~ n, labeller = label_both, scales = "free_x") +
    
    labs(
      title = "Snapshots of the Berry-Esseen Error Term F_n(x) - Φ(x)",
      subtitle = "Showing convergence to zero as sample size (n) increases",
      x = "Standardized Value (x)",
      y = "Error Term"
    ) +
    theme_minimal() +
    theme(strip.text = element_text(size = 10)) # Adjust facet label size if needed

  # Return Plots 
  return(list(error_vs_n_plot = p_error_vs_n, error_vs_x_plot = p_error_vs_x))
}
```



```{r}
be_plots_snapshots <- simulate_berry_esseen_snapshots()
```
 
Note that the shape factor ($\varepsilon$) for **uniform** is much smaller than that of **exponential**, thus denouncing the pronounced skewness of this latter distribution. In particular, this indicates that the convergence toward a normal for **exponential** should be much slower.  

```{r}
be_plots_snapshots$error_vs_n_plot
```


This plot now shows two panels (with distinct vertical scales). When comparing them, we can clearly see the hierarchy of convergence:

  - **Uniform**: The observed error (blue line) is very low and well-behaved, staying  far below its theoretical bound.
  - **Exponential**: The observed error is a bit higher, demonstrating a slower convergence. It starts much larger, confirming that the high skewness of the exponential parent makes the normal approximation very deficient at small $n$. Also, the violation of the theoretical bound (in red) is more severe and persists for larger $n$.
  

```{r}
be_plots_snapshots$error_vs_x_plot
```


This plot is a 2x3 grid which beautifully illustrates the diversity in convergence:

  - Top Row (**Exponential**): The error wave is huge, highly asymmetric, and shrinks slowly.
  
  - Bottom Row (**Uniform**): The error wave is small, reasonably symmetric, and shrinks rapidly.


We may summarize the grid as follows:

  - Reading across any row shows the convergence for that distribution. 
  - Reading down any column provides a direct comparison of the error structure across the distributions at a fixed sample size. 
  
In conclusion, this final visualization effectively illustrates **Berry-Esseen 
theorem** in action.


#### References

