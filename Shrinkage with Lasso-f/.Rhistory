library(glmnet)
set.seed(123)
n_obs <- 100
n_pred <- 20
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
rnorm(n_obs * n_pred)
matrix(..., nrow = n_obs, ncol = n_pred)
set.seed(123)
n_obs <- 100
n_pred <- 20
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
rnorm(n_obs * n_pred)
set.seed(123)
n_obs <- 100
n_pred <- 20
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
rnorm(n_obs * n_pred)
true_coefficients <- c(2.5, 0, 0, 0, -1.8, 0, 0, 0, 0, 3.1, rep(0, n_pred - 10))
set.seed(123)
n_obs <- 100
n_pred <- 20
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
true_coefficients <- c(2.5, 0, 0, 0, -1.8, 0, 0, 0, 0, 3.1, rep(0, n_pred - 10))
set.seed(123)
n_obs <- 100
n_pred <- 20
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
true_coefficients <- c(2.5, 0, 0, 0, -1.8, 0, 0, 0, 0, 3.1, rep(0, n_pred - 10))
y <- x %*% true_coefficients + rnorm(n_obs, sd = 0.5)
lasso_model <- glmnet(x, y, alpha = 1)
str(lasso_model)
print(lasso_model)
plot(lasso_model, xvar = "lambda", label = TRUE)
cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10)
plot(cv_lasso)
cv_lasso$lambda.min
cv_lasso$lambda.1se
coef(lasso_model, s = lambda_min)
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
coef(lasso_model, s = lambda_min)
coef(lasso_model, s = lambda_1se)
predictions_min <- predict(lasso_model, newx = new_x, s = lambda_min)
n_new <- 5
set.seed(456)
new_x <- matrix(rnorm(n_new * n_pred), nrow = n_new, ncol = n_pred)
predictions_min <- predict(lasso_model, newx = new_x, s = lambda_min)
predictions_1se <- predict(lasso_model, newx = new_x, s = lambda_1se)
# View the predictions
cat("Predictions using lambda.min:\n")
print(predictions_min)
cat("\nPredictions using lambda.1se:\n")
print(predictions_1se)
# Predicting on the original training data 'x'
predictions_on_training_data <- predict(lasso_model, newx = x, s = lambda_min)
# print(head(predictions_on_training_data))
# Predicting on the original training data 'x'
predictions_on_training_data <- predict(lasso_model, newx = x, s = lambda_min)
print(head(predictions_on_training_data))
ss_total <- sum((y - mean(y))^2)
ss_residual_min <- sum((y - predicted_y_min)^2)
# set.seed(123): This function sets the "seed" for R's random number generator.
# By setting a seed, you ensure that any subsequent operations involving random numbers will produce the exact same results.
# This is crucial for reproducibility.
set.seed(123)
# n_obs <- 100: Defines the number of observations (rows) in our dataset.
n_obs <- 100
# n_pred <- 20: Defines the number of predictor variables (columns).
n_pred <- 20
# x <- matrix(rnorm(n_obs * n_pred), ...): Generates a matrix of predictor variables (our 'x' data).
# It contains 100 rows and 20 columns, with values drawn from a standard normal distribution.
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
# true_coefficients <- c(...): Creates a vector of "true" coefficients.
# We intentionally set most of these to zero to simulate a situation where only predictors 1, 5, and 10 are actually important.
true_coefficients <- c(2.5, 0, 0, 0, -1.8, 0, 0, 0, 0, 3.1, rep(0, n_pred - 10))
# y <- x %*% true_coefficients + rnorm(n_obs, sd = 0.5): Generates our response variable 'y'.
# It's a linear combination of the predictors and true coefficients, plus some random noise to simulate a real-world scenario.
y <- x %*% true_coefficients + rnorm(n_obs, sd = 0.5)
cat("Simulated data created with 100 observations and 20 predictors.\n")
# lasso_model <- glmnet(x, y, alpha = 1): This is the core function call to fit the LASSO model.
# `alpha = 1` specifies LASSO regression (L1 penalty).
# `alpha = 0` would be Ridge regression, and 0 < alpha < 1 is Elastic Net.
lasso_model <- glmnet(x, y, alpha = 1)
# print(lasso_model): Displays a summary of the fitted `glmnet` object.
# It shows the number of non-zero coefficients (Df), the % of deviance explained, and the lambda value for each step.
print(lasso_model)
# plot(lasso_model, xvar = "lambda", label = TRUE): Generates a plot showing the regularization path of the coefficients.
# As lambda increases (moving left on the plot), the penalty strengthens, and coefficients are shrunk to zero.
plot(lasso_model, xvar = "lambda", label = TRUE)
# cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10): This performs 10-fold cross-validation to find the optimal lambda value.
cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10)
# plot(cv_lasso): Plots the cross-validation results.
# The y-axis is the Mean Squared Error (MSE), and the x-axis is log(lambda).
# It helps us visualize the lambda that results in the lowest prediction error.
plot(cv_lasso)
# cv_lasso$lambda.min: Extracts the lambda value corresponding to the minimum cross-validated error.
lambda_min <- cv_lasso$lambda.min
# cv_lasso$lambda.1se: Extracts the lambda value corresponding to the "one-standard error" rule.
lambda_1se <- cv_lasso$lambda.1se
cat("Optimal lambda (lambda.min):", lambda_min, "\n")
cat("Parsimonious lambda (lambda.1se):", lambda_1se, "\n")
# coef(lasso_model, s = lambda_min): Extracts the regression coefficients from the model for a specific lambda value.
# We will inspect the coefficients for both lambda.min and lambda.1se.
coef_min <- coef(lasso_model, s = lambda_min)
coef_1se <- coef(lasso_model, s = lambda_1se)
cat("\nCoefficients using lambda.min:\n")
print(coef_min)
cat("\nCoefficients using lambda.1se:\n")
print(coef_1se)
# First, we need to create the 'new_x' object. It must have the same number of columns as the original 'x' matrix.
# Let's generate 5 new observations to predict on.
n_new <- 5
set.seed(456) # A different seed for the new data
new_x <- matrix(rnorm(n_new * n_pred), nrow = n_new, ncol = n_pred)
# predict(lasso_model, newx = new_x, s = lambda_min): Uses the trained model to make predictions on the new data matrix `new_x`.
# `s` specifies which lambda value (and thus which version of the model) to use for the prediction.
predictions_min <- predict(lasso_model, newx = new_x, s = lambda_min)
cat("Predictions for 5 new observations using lambda.min:\n")
print(predictions_min)
# This section calculates the R-squared value to assess how well the model explains the variability in the response.
# predicted_y_min <- predict(lasso_model, newx = x, s = lambda_min): Predicts the `y` values using the original `x` data.
predicted_y_min <- predict(lasso_model, newx = x, s = lambda_min)
# ss_total <- sum((y - mean(y))^2): Calculates the Total Sum of Squares (SST).
ss_total <- sum((y - mean(y))^2)
# ss_residual_min <- sum((y - predicted_y_min)^2): Calculates the Residual Sum of Squares (SSR).
ss_residual_min <- sum((y - predicted_y_min)^2)
# r_squared_min <- 1 - (ss_residual_min / ss_total): The formula for R-squared.
r_squared_min <- 1 - (ss_residual_min / ss_total)
cat("R-squared for the lambda.min model:", round(r_squared_min, 4), "\n")
ols_model <- lm(y ~ x)
ols_model <- lm(y ~ x)
summary(ols_model)
ols_model_adj <- (y ~ x1 + x5 + x10)
ols_model_adj <- (y ~ x1 + x5 + x10)
print(ols_model_adj)
ols_model_adj <- (y ~ x1 + x5 + x10)
summary(ols_model_adj)
ols_model_adj <- lm(y ~ x1 + x5 + x10)
x[1]
ols_model_adj <- lm(y ~ x[1] + x[5] + x[10])
ols_model_adj <- lm(y ~ x[1,] + x[5,] + x[10,])
x[1,]
x[,1]
ols_model_adj <- lm(y ~ x[,1] + x[,5] + x[,10])
summary(ols_model_adj)
ols_model_adj <- lm(y ~ x[,1] + x[,5] + x[,10])
summary(ols_model_adj)
#' Simulate Data for LASSO Regression Example
#'
#' @description This function generates a simulated dataset suitable for demonstrating LASSO regression.
#' It creates a predictor matrix 'x', a response vector 'y', and the 'true' coefficients used
#' in the data generation process, where only a few predictors are truly influential.
#'
#' @param n_obs Integer. The number of observations (rows) to generate. Default is 100.
#' @param n_pred Integer. The number of predictor variables (columns) to generate. Must be at least 10 for this specific setup. Default is 20.
#' @param seed Integer. A random seed for reproducibility. Default is 123.
#'
#' @return A list containing three elements:
#' \item{x}{A matrix of predictor variables with dimensions n_obs x n_pred.}
#' \item{y}{A numeric vector of the response variable.}
#' \item{true_coefficients}{The vector of coefficients used to generate y.}
simulate_lasso_data <- function(n_obs = 100, n_pred = 20, seed = 123) {
# Set the seed for reproducibility inside the function
set.seed(seed)
# Check for valid number of predictors for this simulation's logic
if (n_pred < 10) {
stop("Please use n_pred >= 10 for this simulation's coefficient setup.")
}
# Generate the predictor matrix from a standard normal distribution
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
# Create the true coefficients vector in a robust way
# Start with all zeros, then assign the non-zero coefficients
true_coefficients <- rep(0, n_pred)
true_coefficients[1] <- 2.5
true_coefficients[5] <- -1.8
true_coefficients[10] <- 3.1
# Generate the response variable y: y = X * beta + noise
y <- x %*% true_coefficients + rnorm(n_obs, sd = 0.5)
# Return all relevant objects in a named list
return(list(x = x, y = y, true_coefficients = true_coefficients))
}
# --- Now, call the function to generate our data ---
# Use the default values (100 observations, 20 predictors, seed = 123)
sim_data <- simulate_lasso_data()
# Extract the components from the list into our environment for use in later chunks
x <- sim_data$x
y <- sim_data$y
true_coefficients <- sim_data$true_coefficients
cat(paste("Simulated data created using a function with",
nrow(x), "observations and", ncol(x), "predictors.\n"))
ols_model <- lm(y ~ x)
# First, we need to create the 'new_x' object. It must have the same number of columns as the original 'x' matrix.
# Let's generate 5 new observations to predict on.
n_new <- 5
set.seed(456) # A different seed for the new data
n_pred <- n_pred
new_x <- matrix(rnorm(n_new * n_pred), nrow = n_new, ncol = n_pred)
# predict(lasso_model, newx = new_x, s = lambda_min): Uses the trained model to make predictions on the new data matrix `new_x`.
# `s` specifies which lambda value (and thus which version of the model) to use for the prediction.
predictions_min <- predict(lasso_model, newx = new_x, s = lambda_min)
library(glmnet)
#' Simulate Data for LASSO Regression Example
#'
#' @description This function generates a simulated dataset suitable for demonstrating LASSO regression.
#' It creates a predictor matrix 'x', a response vector 'y', and the 'true' coefficients used
#' in the data generation process, where only a few predictors are truly influential.
#'
#' @param n_obs Integer. The number of observations (rows) to generate. Default is 100.
#' @param n_pred Integer. The number of predictor variables (columns) to generate. Must be at least 10 for this specific setup. Default is 20.
#' @param seed Integer. A random seed for reproducibility. Default is 123.
#'
#' @return A list containing three elements:
#' \item{x}{A matrix of predictor variables with dimensions n_obs x n_pred.}
#' \item{y}{A numeric vector of the response variable.}
#' \item{true_coefficients}{The vector of coefficients used to generate y.}
simulate_lasso_data <- function(n_obs = 100, n_pred = 20, seed = 123) {
# Set the seed for reproducibility inside the function
set.seed(seed)
# Check for valid number of predictors for this simulation's logic
if (n_pred < 10) {
stop("Please use n_pred >= 10 for this simulation's coefficient setup.")
}
# Generate the predictor matrix from a standard normal distribution
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
# Create the true coefficients vector in a robust way
# Start with all zeros, then assign the non-zero coefficients
true_coefficients <- rep(0, n_pred)
true_coefficients[1] <- 2.5
true_coefficients[5] <- -1.8
true_coefficients[10] <- 3.1
# Generate the response variable y: y = X * beta + noise
y <- x %*% true_coefficients + rnorm(n_obs, sd = 0.5)
# Return all relevant objects in a named list
return(list(x = x, y = y, true_coefficients = true_coefficients))
}
# --- Now, call the function to generate our data ---
# Use the default values (100 observations, 20 predictors, seed = 123)
sim_data <- simulate_lasso_data()
# Extract the components from the list into our environment for use in later chunks
x <- sim_data$x
y <- sim_data$y
true_coefficients <- sim_data$true_coefficients
cat(paste("Simulated data created using a function with",
nrow(x), "observations and", ncol(x), "predictors.\n"))
ols_model <- lm(y ~ x)
summary(ols_model)
ols_model_adj <- lm(y ~ x[,1] + x[,5] + x[,10])
summary(ols_model_adj)
# lasso_model <- glmnet(x, y, alpha = 1): This is the core function call to fit the LASSO model.
# `alpha = 1` specifies LASSO regression (L1 penalty).
# `alpha = 0` would be Ridge regression, and 0 < alpha < 1 is Elastic Net.
lasso_model <- glmnet(x, y, alpha = 1)
# print(lasso_model): Displays a summary of the fitted `glmnet` object.
# It shows the number of non-zero coefficients (Df), the % of deviance explained, and the lambda value for each step.
print(lasso_model)
# plot(lasso_model, xvar = "lambda", label = TRUE): Generates a plot showing the regularization path of the coefficients.
# As lambda increases (moving left on the plot), the penalty strengthens, and coefficients are shrunk to zero.
plot(lasso_model, xvar = "lambda", label = TRUE)
# cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10): This performs 10-fold cross-validation to find the optimal lambda value.
cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10)
# plot(cv_lasso): Plots the cross-validation results.
# The y-axis is the Mean Squared Error (MSE), and the x-axis is log(lambda).
# It helps us visualize the lambda that results in the lowest prediction error.
plot(cv_lasso)
library(glmnet)
#' Simulate Data for LASSO Regression Example
#'
#' @description This function generates a simulated dataset suitable for demonstrating LASSO regression.
#' It creates a predictor matrix 'x', a response vector 'y', and the 'true' coefficients used
#' in the data generation process, where only a few predictors are truly influential.
#'
#' @param n_obs Integer. The number of observations (rows) to generate. Default is 100.
#' @param n_pred Integer. The number of predictor variables (columns) to generate. Must be at least 10 for this specific setup. Default is 20.
#' @param seed Integer. A random seed for reproducibility. Default is 123.
#'
#' @return A list containing three elements:
#' \item{x}{A matrix of predictor variables with dimensions n_obs x n_pred.}
#' \item{y}{A numeric vector of the response variable.}
#' \item{true_coefficients}{The vector of coefficients used to generate y.}
simulate_lasso_data <- function(n_obs = 100, n_pred = 20, seed = 123) {
# Set the seed for reproducibility inside the function
set.seed(seed)
# Check for valid number of predictors for this simulation's logic
if (n_pred < 10) {
stop("Please use n_pred >= 10 for this simulation's coefficient setup.")
}
# Generate the predictor matrix from a standard normal distribution
x <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
# Create the true coefficients vector in a robust way
# Start with all zeros, then assign the non-zero coefficients
true_coefficients <- rep(0, n_pred)
true_coefficients[1] <- 2.5
true_coefficients[5] <- -1.8
true_coefficients[10] <- 3.1
# Generate the response variable y: y = X * beta + noise
y <- x %*% true_coefficients + rnorm(n_obs, sd = 0.5)
# Return all relevant objects in a named list
return(list(x = x, y = y, true_coefficients = true_coefficients))
}
# --- Now, call the function to generate our data ---
# Use the default values (100 observations, 20 predictors, seed = 123)
sim_data <- simulate_lasso_data()
# Extract the components from the list into our environment for use in later chunks
x <- sim_data$x
y <- sim_data$y
true_coefficients <- sim_data$true_coefficients
cat(paste("Simulated data created using a function with",
nrow(x), "observations and", ncol(x), "predictors.\n"))
ols_model <- lm(y ~ x)
summary(ols_model)
ols_model_adj <- lm(y ~ x[,1] + x[,5] + x[,10])
summary(ols_model_adj)
# lasso_model <- glmnet(x, y, alpha = 1): This is the core function call to fit the LASSO model.
# `alpha = 1` specifies LASSO regression (L1 penalty).
# `alpha = 0` would be Ridge regression, and 0 < alpha < 1 is Elastic Net.
lasso_model <- glmnet(x, y, alpha = 1)
# print(lasso_model): Displays a summary of the fitted `glmnet` object.
# It shows the number of non-zero coefficients (Df), the % of deviance explained, and the lambda value for each step.
print(lasso_model)
# plot(lasso_model, xvar = "lambda", label = TRUE): Generates a plot showing the regularization path of the coefficients.
# As lambda increases (moving left on the plot), the penalty strengthens, and coefficients are shrunk to zero.
plot(lasso_model, xvar = "lambda", label = TRUE)
# cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10): This performs 10-fold cross-validation to find the optimal lambda value.
cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10)
# plot(cv_lasso): Plots the cross-validation results.
# The y-axis is the Mean Squared Error (MSE), and the x-axis is log(lambda).
# It helps us visualize the lambda that results in the lowest prediction error.
plot(cv_lasso)
# First, we need to create the 'new_x' object.
# Let's generate 5 new observations to predict on.
n_new <- 5
set.seed(456) # A different seed for the new data
# ---- THE FIX IS HERE ----
# Instead of using the non-existent 'n_pred', we get the number of columns
# directly from our training data matrix 'x'. This is much more robust.
num_predictors <- ncol(x)
new_x <- matrix(rnorm(n_new * num_predictors), nrow = n_new, ncol = num_predictors)
# predict(lasso_model, newx = new_x, s = lambda_min): Uses the trained model to make predictions.
# `s` specifies which lambda value (and thus which version of the model) to use for the prediction.
predictions_min <- predict(lasso_model, newx = new_x, s = lambda_min)
cat("Number of predictors in training data:", num_predictors, "\n")
cat("Dimensions of new_x matrix:", dim(new_x), "\n\n")
cat("Predictions for 5 new observations using lambda.min:\n")
print(predictions_min)
library(tidyverse)
library(glmnet)
